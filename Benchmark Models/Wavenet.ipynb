{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                                           padding, dilation, groups, bias)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = super(CausalConv1d, self).forward(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedConv1d(nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(DilatedConv1d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                                            padding, dilation, groups, bias)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = super(DilatedConv1d, self).forward(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, dilation, stride):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.filter_conv = DilatedConv1d(in_channels=res_channels, out_channels=res_channels, dilation=dilation, stride=stride)\n",
    "        self.gate_conv = DilatedConv1d(in_channels=res_channels, out_channels=res_channels, dilation=dilation, stride=stride)\n",
    "        self.skip_conv = nn.Conv1d(in_channels=res_channels, out_channels=skip_channels, kernel_size=1)\n",
    "        self.residual_conv = nn.Conv1d(in_channels=res_channels, out_channels=res_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        sigmoid_out = torch.sigmoid(self.gate_conv(inputs))\n",
    "        tahn_out = torch.tanh(self.filter_conv(inputs))\n",
    "        output = sigmoid_out * tahn_out\n",
    "        \n",
    "        skip_out = self.skip_conv(output)\n",
    "        res_out = self.residual_conv(output)\n",
    "        res_out = res_out + inputs[:, :, -res_out.size(2):]\n",
    "        # res\n",
    "        return res_out , skip_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_depth=1, res_channels=32, skip_channels=128, dilation_depth=4, n_repeat=3, stride=1):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilations = [2**i for i in range(dilation_depth)] * n_repeat\n",
    "        self.main = nn.ModuleList([ResidualBlock(res_channels,skip_channels,dilation, stride=stride) for dilation in self.dilations])\n",
    "        self.pre_conv = CausalConv1d(in_channels=in_depth, out_channels=res_channels)\n",
    "        self.post = nn.Sequential(nn.ReLU(),\n",
    "                                  nn.Conv1d(skip_channels,in_depth,1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(35, 12))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        outputs = self.preprocess(inputs)\n",
    "        skip_connections = []\n",
    "        \n",
    "        cnt = 0\n",
    "        for layer in self.main:\n",
    "            outputs,skip = layer(outputs)\n",
    "            cnt += 1\n",
    "            skip_connections.append(skip)\n",
    "            \n",
    "        outputs = sum([s[:,:,-outputs.size(2):] for s in skip_connections])\n",
    "        outputs = self.post(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def preprocess(self,inputs):\n",
    "        out = self.pre_conv(inputs)\n",
    "        return out\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        par = list(self.parameters())\n",
    "        s = sum([np.prod(list(d.size())) for d in par])\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('../dataset/pems.npy', allow_pickle=True).item()\n",
    "x, y = dataset['X'], dataset['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(x.shape[0], x.shape[2])\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "x = scaler.transform(x)\n",
    "x = x.reshape(x.shape[0], 1, x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Data.TensorDataset(x_train, y_train)\n",
    "loader = Data.DataLoader(dataset=dataset_train, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "batch_num = 0\n",
    "for step, (batch_x, batch_y) in enumerate(loader):\n",
    "    batch_num += 1\n",
    "print('%d batches' % batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = choose_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, y, net, criterion=nn.MSELoss(), clips=12, suppress_output=False):\n",
    "    y_hat = net(x.to(device))\n",
    "    y_hat = y_hat[:, :, :clips]\n",
    "    y = y[:, :, :clips].to(device)\n",
    "    loss = criterion(y_hat, y)\n",
    "    RMSE = loss.item() ** 0.5\n",
    "    MAPE = compute_MAPE(y, y_hat)\n",
    "    MAE = compute_MAE(y, y_hat)\n",
    "    if suppress_output == False:\n",
    "        print('samples: %d - %d\\ntime clips: %d\\nRMSE: %.2f\\nMAPE: %.2f%%\\nMAE: %.2f' % (y_hat.shape[0], y.shape[0], clips, RMSE, MAPE, MAE))\n",
    "    return RMSE, MAPE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(rounds=20, epochs=150, lr=0.001, weight_decay=0.001, clips=[3, 6, 12], criterion=nn.MSELoss()):\n",
    "    # init\n",
    "    criterions = ['RMSE', 'MAPE', 'MAE']\n",
    "    columns = []\n",
    "    for clip in clips:\n",
    "        for each in criterions:\n",
    "            columns.append('%s_%d' % (each, clip))\n",
    "    df = []\n",
    "    # multiple runs\n",
    "    for cur_round in range(1, rounds + 1, 1):\n",
    "        # init\n",
    "        net = WaveNet().to(device)        \n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train_losses, test_losses = [], []\n",
    "        # train\n",
    "        for epoch in range(epochs):\n",
    "            t_start = time.time()\n",
    "            running_RMSE, running_MAPE = 0, 0\n",
    "\n",
    "            for step, (batch_x, batch_y) in enumerate(loader):\n",
    "                input_x = batch_x.to(device)\n",
    "                y = batch_y.to(device)  \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output_data = net(input_x)\n",
    "                loss = criterion(output_data, y)\n",
    "                MAPE = compute_MAPE(output_data, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_RMSE += loss.item() ** 0.5\n",
    "                running_MAPE += MAPE\n",
    "            train_losses.append(running_RMSE / batch_num)\n",
    "\n",
    "            test_losses.append(criterion(net(x_test.to(device)), y_test.to(device)).item() ** 0.5)\n",
    "            t_end = time.time()\n",
    "\n",
    "            print('\\rround=%02d, epoch=%d, RMSE=%.2f, MAPE=%.2f%%, time=%.2fs per epoch      ' \\\n",
    "                  % (cur_round, epoch+1, running_RMSE / batch_num, running_MAPE / batch_num, t_end-t_start), end='')\n",
    "        print()\n",
    "        #evaluate\n",
    "        evaluation_clip = []\n",
    "        for clip in clips:\n",
    "            RMSE, MAPE, MAE = evaluate(x_test, y_test, net, criterion, clips=clip, suppress_output=True)\n",
    "            eva = [RMSE, MAPE, MAE]\n",
    "            evaluation_clip += eva\n",
    "        df.append(evaluation_clip)\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df.index = np.arange(1, rounds+1, 1)\n",
    "    return df, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, train_losses, test_losses = run(rounds=10, epochs=150, lr=0.6, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss = pd.DataFrame({'epoch': np.arange(1, len(train_losses)+1, 1), 'train_loss': train_losses, 'test_loss': test_losses})\n",
    "sns.lineplot(x='epoch', y='train_loss', data=dloss, label='train loss')\n",
    "sns.lineplot(x='epoch', y='test_loss', data=dloss, label='test loss')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Multiple Run Results/WaveNet_10.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
